{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych\n",
    "## Pobranie danych z kafki i przygotowanie dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = spark.readStream.format('kafka'). \\\n",
    "    option('kafka.bootstrap.servers', 'master,slave01'). \\\n",
    "    option('subscribe', 'final_open_weather_mw_v2'). \\\n",
    "    option('startingOffsets', 'earliest'). \\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = binary_data.selectExpr('CAST(key AS STRING)', \"split(value, '[|]')[0] as current_temp\", \"split(value, '[|]')[1] as feels_like\", 'partition', 'offset', 'timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zamienienie kolumny 'current_temp' i 'feels_like' z typu string na double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "dataframe = dataframe.withColumn('current_temp', dataframe.current_temp.cast(DoubleType()))\n",
    "dataframe = dataframe.withColumn('feels_like', dataframe.feels_like.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potrzebuję teraz jeszcze dodatkowej kolumny \"dnia\" do późniejszych operacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn('yyyy_mm_dd', dataframe.timestamp.substr(1, 10)) #Wyciągnięcie yyyy-mm-dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtracja danych, biorę tylko te dane, które został odebrane w ciągu ostatnich 5 dni. Zostało to rozwiązane na dwa sposoby - I zakomentowany polega na różnicy timestampa otrzymanego z akutalnym i przyrównania go do:\n",
    "### 5 dni na s -> 432000\n",
    "\n",
    "### Drugi sposób to wykorzystanie metody 'DataDiff', a następnie na podsatwie tej wartości określamy ostatnie 5 dni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#dataframe = dataframe.withColumn('timestamp_in_ms', F.unix_timestamp(dataframe.timestamp)).withColumn('timestamp_current', F.unix_timestamp(F.current_timestamp()))\n",
    "#dataframe = dataframe.filter((dataframe.timestamp_current - dataframe.timestamp_in_ms) <= 432000)\n",
    "\n",
    "dataframe = dataframe.withColumn('date_diff', F.datediff(F.current_timestamp(), dataframe.yyyy_mm_dd))\n",
    "dataframe = dataframe.filter((dataframe.date_diff >= 1) & (dataframe.date_diff <= 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyznaczenie dla każdego rekordu różnicę między temp. Aktualną, a temperaturą odczuwalną\n",
    "## Dodatkowo wyciągnięcie z niej wartości bezwzględnej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "dataframe = dataframe.withColumn('temp_diff', abs(F.round((dataframe.current_temp - dataframe.feels_like), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataframe.writeStream.format('memory').queryName('weather_data_v2').outputMode('append').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+----+\n",
      "|   key|yyyy_mm_dd|temp_diff_avg|rank|\n",
      "+------+----------+-------------+----+\n",
      "|Warsaw|2020-05-25|          4.8|   1|\n",
      "|Krakow|2020-05-25|         3.81|   2|\n",
      "+------+----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT w.key, w.yyyy_mm_dd, w.temp_diff_avg, w.rank \\\n",
    "           FROM ( \\\n",
    "               SELECT q.key, q.yyyy_mm_dd, q.temp_diff_avg, RANK() OVER (PARTITION BY q.yyyy_mm_dd ORDER BY q.temp_diff_avg DESC) as rank\\\n",
    "               FROM ( \\\n",
    "                   Select key, Round(avg(temp_diff),2) as temp_diff_avg, yyyy_mm_dd \\\n",
    "                   FROM weather_data_v2 \\\n",
    "                   Group by key, yyyy_mm_dd) as q \\\n",
    "                ) as w \\\n",
    "            WHERE w.rank <= 3 \\\n",
    "            ORDER BY yyyy_mm_dd, rank \\\n",
    "            ').show()\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+---------+------+-------------------+----------+---------+---------+\n",
      "|   key|current_temp|feels_like|partition|offset|          timestamp|yyyy_mm_dd|date_diff|temp_diff|\n",
      "+------+------------+----------+---------+------+-------------------+----------+---------+---------+\n",
      "|Warsaw|      288.41|    283.56|        0|     0|2020-05-25 13:24:16|2020-05-25|        0|     4.85|\n",
      "|Warsaw|      288.91|    284.16|        0|     1|2020-05-25 13:34:27|2020-05-25|        0|     4.75|\n",
      "|Krakow|      285.74|    281.93|        0|     2|2020-05-25 13:41:04|2020-05-25|        0|     3.81|\n",
      "+------+------------+----------+---------+------+-------------------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * from weather_data_v2\\\n",
    "            ').show(800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+---------+------+-------------------+----------+---------+---------+\n",
      "|   key|current_temp|feels_like|partition|offset|          timestamp|yyyy_mm_dd|date_diff|temp_diff|\n",
      "+------+------------+----------+---------+------+-------------------+----------+---------+---------+\n",
      "|Warsaw|      288.41|    283.56|        0|     0|2020-05-25 13:24:16|2020-05-25|        0|     4.85|\n",
      "|Warsaw|      288.91|    284.16|        0|     1|2020-05-25 13:34:27|2020-05-25|        0|     4.75|\n",
      "|Krakow|      285.74|    281.93|        0|     2|2020-05-25 13:41:04|2020-05-25|        0|     3.81|\n",
      "+------+------------+----------+---------+------+-------------------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * from weather_data_v2\\\n",
    "            ').show(800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[IPyKernel] PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
